{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "82cfe4e1",
      "metadata": {},
      "source": [
        "# AI + Knowledge & Creativity\n",
        "\n",
        "This notebook contains four small, self-contained projects using free/open-source tools:\n",
        "\n",
        "1. **AI Historian** — reconstruct events with RAG-style retrieval from Wikipedia (FAISS + sentence-transformers)\n",
        "2. **Myth vs Fact AI** — split user text into claims and perform simple verification via Wikipedia\n",
        "3. **AI Story Continuator** — continue a user story in a chosen author's style using a small HF model\n",
        "4. **AI Music Mood Composer** — detect mood from text and generate a simple MIDI melody\n",
        "\n",
        "Each section includes runnable cells. This notebook is meant as a starter/prototype — swap models or extend components as needed.\n",
        "\n",
        "⚠️ Notes:\n",
        "- Some cells will download ML models on first run (transformers, sentence-transformers). These can be large.\n",
        "- If you're running in Colab, prefix pip installs with `!` in a cell or use `%pip install`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a309dbeb",
      "metadata": {},
      "source": [
        "## 0 — Install dependencies\n",
        "Run this cell once to install required packages. If you are in Google Colab, run as-is. In local Jupyter, you can run it from the notebook (it may take several minutes).\n",
        "\n",
        "- The installs are conservative: we avoid heavy TensorFlow/Magenta by using a simple MIDI generator (pretty_midi).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b8be3c",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --quiet transformers sentence-transformers faiss-cpu wikipedia pretty_midi librosa soundfile torch==2.0.1 tqdm\n",
        "\n",
        "# Note: If you're in an environment that already has these packages, pip will skip reinstalling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "932f358b",
      "metadata": {},
      "source": [
        "## 1 — AI Historian (RAG with Wikipedia + local generator)\n",
        "This section builds a small FAISS index of a few Wikipedia pages, retrieves top pages for a query, and then uses a small text-generation model to produce a reconstruction with inline citations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c2dc151",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import wikipedia\n",
        "import numpy as np\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from transformers import pipeline\n",
        "\n",
        "EMB_MODEL = 'all-MiniLM-L6-v2'  # small, fast\n",
        "INDEX_PATH = Path('hist_vector.index')\n",
        "META_PATH = Path('hist_meta.pkl')\n",
        "\n",
        "class SimpleWikiRetriever:\n",
        "    def __init__(self, index_path=INDEX_PATH, meta_path=META_PATH, emb_model=EMB_MODEL):\n",
        "        self.model = SentenceTransformer(emb_model)\n",
        "        self.index_path = Path(index_path)\n",
        "        self.meta_path = Path(meta_path)\n",
        "        self.index = None\n",
        "        self.meta = []\n",
        "        if self.index_path.exists() and self.meta_path.exists():\n",
        "            self._load()\n",
        "\n",
        "    def build_index(self, titles):\n",
        "        pages = []\n",
        "        for t in titles:\n",
        "            try:\n",
        "                content = wikipedia.page(t).content\n",
        "                pages.append((t, content))\n",
        "            except Exception as e:\n",
        "                print(f'Could not fetch {t}:', e)\n",
        "        texts = [p[1] for p in pages]\n",
        "        self.meta = [p[0] for p in pages]\n",
        "        emb = self.model.encode(texts, show_progress_bar=True)\n",
        "        d = emb.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(d)\n",
        "        self.index.add(np.array(emb).astype('float32'))\n",
        "        faiss.write_index(self.index, str(self.index_path))\n",
        "        with open(self.meta_path, 'wb') as f:\n",
        "            pickle.dump(self.meta, f)\n",
        "        print('Index built with', len(self.meta), 'pages')\n",
        "\n",
        "    def _load(self):\n",
        "        self.index = faiss.read_index(str(self.index_path))\n",
        "        with open(self.meta_path, 'rb') as f:\n",
        "            self.meta = pickle.load(f)\n",
        "\n",
        "    def query(self, text, k=4):\n",
        "        emb = self.model.encode([text]).astype('float32')\n",
        "        D, I = self.index.search(emb, k)\n",
        "        results = []\n",
        "        for i in I[0]:\n",
        "            if i < len(self.meta):\n",
        "                title = self.meta[i]\n",
        "                try:\n",
        "                    summary = wikipedia.summary(title, sentences=3)\n",
        "                except Exception:\n",
        "                    summary = ''\n",
        "                results.append({'title': title, 'summary': summary})\n",
        "        return results\n",
        "\n",
        "# Example usage: build a small index (run once)\n",
        "seed_titles = [\n",
        "    'French Revolution', 'Industrial Revolution', 'World War II',\n",
        "    'Fall of the Western Roman Empire', 'American Civil War'\n",
        "]\n",
        "retriever = SimpleWikiRetriever()\n",
        "if not retriever.index:\n",
        "    retriever.build_index(seed_titles)\n",
        "\n",
        "# Query function that uses a small generator\n",
        "generator = pipeline('text-generation', model='gpt2', device=-1)\n",
        "\n",
        "def reconstruct_event(query):\n",
        "    docs = retriever.query(query, k=4)\n",
        "    context = '\\n\\n'.join([f\"{d['title']}: {d['summary']}\" for d in docs])\n",
        "    prompt = f\"Reconstruct the historical event: {query}\\nUse these sources:\\n{context}\\n\\nWrite a concise, sourced reconstruction (cite titles inline).\"\n",
        "    out = generator(prompt, max_length=300, num_return_sequences=1)[0]['generated_text']\n",
        "    return out\n",
        "\n",
        "# Try a sample query\n",
        "print(reconstruct_event('Causes of the French Revolution'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e55f7009",
      "metadata": {},
      "source": [
        "## 2 — Myth vs Fact\n",
        "Split text into claims and verify them with a quick Wikipedia-based heuristic. This is a prototype — for production use, integrate fact-check corpora and claim-checking models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d403eff",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import wikipedia\n",
        "\n",
        "\n",
        "def split_claims(text):\n",
        "    sents = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
        "    return [s for s in sents if len(s) > 8]\n",
        "\n",
        "\n",
        "def verify_claim_via_wikipedia(claim):\n",
        "    try:\n",
        "        hits = wikipedia.search(claim, results=3)\n",
        "        if not hits:\n",
        "            return {'claim': claim, 'verdict': 'unknown', 'evidence': []}\n",
        "        evidence = []\n",
        "        for h in hits:\n",
        "            try:\n",
        "                sm = wikipedia.summary(h, sentences=2)\n",
        "                evidence.append({'title': h, 'summary': sm, 'url': wikipedia.page(h).url})\n",
        "            except Exception:\n",
        "                continue\n",
        "        # simple heuristic: if many query words appear in a hit summary, mark likely_true\n",
        "        score = 0\n",
        "        key_tokens = [t.lower() for t in claim.split() if len(t) > 3][:6]\n",
        "        for e in evidence:\n",
        "            txt = e['summary'].lower()\n",
        "            if sum(1 for k in key_tokens if k in txt) >= max(1, len(key_tokens)//3):\n",
        "                score += 1\n",
        "        verdict = 'likely_true' if score >= 1 else 'uncertain'\n",
        "        return {'claim': claim, 'verdict': verdict, 'evidence': evidence}\n",
        "    except Exception as ex:\n",
        "        return {'claim': claim, 'verdict': 'error', 'error': str(ex), 'evidence': []}\n",
        "\n",
        "# Demo\n",
        "sample = 'The pyramids were built by aliens. The moon landing was staged. The French Revolution began in 1789.'\n",
        "claims = split_claims(sample)\n",
        "for c in claims:\n",
        "    res = verify_claim_via_wikipedia(c)\n",
        "    print('\\nCLAIM:', res['claim'])\n",
        "    print('VERDICT:', res['verdict'])\n",
        "    for ev in res['evidence']:\n",
        "        print('-', ev['title'], '-', ev['url'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e2e643",
      "metadata": {},
      "source": [
        "## 3 — AI Story Continuator\n",
        "Use a small GPT-Neo model to continue a story. Optionally, provide short example excerpts to emulate an author's style. For copyright safety, use short public-domain excerpts as style examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a325193c",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = 'EleutherAI/gpt-neo-125M'\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Using device', device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(device)\n",
        "\n",
        "\n",
        "def continue_story(prompt, author_examples=None, max_new_tokens=200):\n",
        "    if author_examples:\n",
        "        full = f\"Author examples:\\n{author_examples}\\n\\nStory start:\\n{prompt}\\n\\nContinuation:\" \n",
        "    else:\n",
        "        full = f\"Story start:\\n{prompt}\\n\\nContinuation:\"\n",
        "    inputs = tokenizer(full, return_tensors='pt').to(device)\n",
        "    out = model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=True, top_p=0.95, temperature=0.9)\n",
        "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    # Return only continuation portion (after the prompt)\n",
        "    return text[len(full):]\n",
        "\n",
        "# Demo\n",
        "start = 'The lanterns shivered in the harbor wind as she stepped onto the dock.'\n",
        "print(continue_story(start, author_examples=None, max_new_tokens=150))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3596b82",
      "metadata": {},
      "source": [
        "## 4 — AI Music Mood Composer (simple MIDI generator)\n",
        "Detect mood from text (using a simple sentiment model) and generate a short MIDI melody using `pretty_midi`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7098f7d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "\n",
        "sent = pipeline('sentiment-analysis')\n",
        "\n",
        "MOOD_SCALES = {\n",
        "    'positive': [60, 62, 64, 65, 67, 69, 71, 72],  # C major\n",
        "    'negative': [60, 62, 63, 65, 67, 68, 70, 72],  # minor-ish\n",
        "}\n",
        "\n",
        "def detect_mood(text):\n",
        "    r = sent(text)[0]\n",
        "    lab = r['label'].lower()\n",
        "    if lab == 'positive':\n",
        "        return 'positive'\n",
        "    elif lab == 'negative':\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'positive'\n",
        "\n",
        "def create_midi_for_mood(mood, filename='mood_melody.mid', length=16):\n",
        "    notes = MOOD_SCALES.get(mood, MOOD_SCALES['positive'])\n",
        "    pm = pretty_midi.PrettyMIDI()\n",
        "    inst = pretty_midi.Instrument(program=0)\n",
        "    start = 0.0\n",
        "    dur = 0.5\n",
        "    for i in range(length):\n",
        "        note_number = notes[i % len(notes)]\n",
        "        note = pretty_midi.Note(velocity=80, pitch=note_number, start=start, end=start+dur)\n",
        "        inst.notes.append(note)\n",
        "        start += dur\n",
        "    pm.instruments.append(inst)\n",
        "    pm.write(filename)\n",
        "    return filename\n",
        "\n",
        "# Demo\n",
        "text = 'I feel upbeat and excited about the new project.'\n",
        "mood = detect_mood(text)\n",
        "print('Detected mood:', mood)\n",
        "fn = create_midi_for_mood(mood, filename='demo_mood.mid')\n",
        "print('MIDI written to', fn)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03b0920",
      "metadata": {},
      "source": [
        "## Wrap-up\n",
        "This notebook gives you a compact, runnable starting point for all four projects.\n",
        "\n",
        "Next steps (suggestions):\n",
        "- Replace `gpt2` / `gpt-neo-125M` with larger HF models if you have GPU access.\n",
        "- For Myth vs Fact, integrate Google FactCheck Tools API or claim-checking datasets (FEVER).\n",
        "- For richer music, incorporate Magenta's MelodyRNN/MusicVAE (requires TensorFlow).\n",
        "\n",
        "If you'd like, I can now:\n",
        "- (A) Add example cells to show how to convert the MIDI to WAV using FluidSynth, or\n",
        "- (B) Split this single notebook into four separate notebooks and provide them as a zip.\n",
        "\n",
        "Tell me which you'd like next, or download the notebook using the link below.\n"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}